{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596567d5-f5f3-4f0b-929c-66cf93f6864e",
   "metadata": {},
   "source": [
    "# Notebook CO2 emissions Agri-food Sector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe51c3-ab4c-46bc-8a31-2b6ff4b28f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install necessary packages \n",
    "!pip install scikit-optimize\n",
    "!pip install shap\n",
    "\n",
    "# General data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistics and data transformation\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # Required for IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Model evaluation and validation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Optimization and hyperparameter tuning\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.callbacks import DeltaYStopper\n",
    "\n",
    "# Explainability tools\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915382a4-3dee-46aa-a4d0-c71402fef895",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfd1c1-5cf0-42e5-be62-f69205b4e9a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data = pd.read_csv(\"Agrofood_co2_emission.csv\")\n",
    "data.info()\n",
    "data.describe()\n",
    "\n",
    "# Unique data points of Area\n",
    "data['Area'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88acafb8-7be7-4b6c-a991-2e5f2b12f62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Range target variable\n",
    "range_C02_emissions = data['total_emission'].max() - data['total_emission'].min()\n",
    "print(range_C02_emissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f5536-e5bd-4e0a-a524-701cd87cacda",
   "metadata": {},
   "source": [
    "### Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e7188-4748-45c7-a2c9-e38bbdfdd457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution numeric features\n",
    "# Select numeric columns\n",
    "columns_n = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "features_n = len(columns_n)\n",
    "\n",
    "# Set up the figure with subplots for each feature\n",
    "fig, axes = plt.subplots(features_n, 1, figsize=(10, features_n * 3))\n",
    "\n",
    "# Plot each feature with histogram and KDE line\n",
    "for i, col in enumerate(columns_n):\n",
    "    sns.histplot(data[col], kde=True, ax=axes[i], color='skyblue')\n",
    "    axes[i].set_title(f'Distribution of {col}', fontsize=14)\n",
    "    axes[i].set_xlabel(col, fontsize=12)\n",
    "    axes[i].set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae8b15-a366-46c4-bc5e-ba0fb9adc2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features to plot against the Total Emission\n",
    "independent_vars = data.columns.difference(['total_emission'])\n",
    "\n",
    "# Scatterplot\n",
    "plt.figure(figsize=(15, 25))  \n",
    "\n",
    "for i, var in enumerate(independent_vars):\n",
    "    plt.subplot(10, 4, i + 1)  \n",
    "    sns.scatterplot(x=data[var], y=data['total_emission'], color= 'skyblue', marker= 'o')\n",
    "    plt.title(f' {var} vs CO2 Emissions')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('CO2 Emissions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edd89a-c299-42a8-9ebb-7bfdeec712c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution plot categoric feature 'Area':\n",
    "distribution_area = data['Area'].value_counts()\n",
    "print(distribution_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0d05a-116a-4679-8db3-375e2760461a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count unique countries\n",
    "unique_countries = data['Area'].nunique()\n",
    "\n",
    "# Count how many areas have exactly 31 values\n",
    "count_31_values = data['Area'].value_counts()[data['Area'].value_counts() == 31].count()\n",
    "\n",
    "print(f\"Number of unique countries: {unique_countries}\")\n",
    "print(f\"Number of areas with exactly 31 observations: {count_31_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bab421-2ad0-4bb1-9896-5d9f9c6f20ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class imbalance in 'Area' feature?\n",
    "# Identify the 'Area' category with the maximum observations\n",
    "max_category = data['Area'].value_counts().idxmax()\n",
    "max_category_count = data['Area'].value_counts().max()\n",
    "\n",
    "print(f\"Category with max observations: {max_category} ({max_category_count} observations)\\n\")\n",
    "       \n",
    "# Calculate differences and filter areas with a difference greater than 10\n",
    "area_counts = data['Area'].value_counts()\n",
    "diff_counts = area_counts.apply(lambda x: max_category_count - x)\n",
    "diff_counts = diff_counts[diff_counts > 10]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "diff_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Areas with Observation Difference Greater Than 10 Compared to Max Observations')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Difference in Observations')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614471bb-5bc7-4b8e-8e4e-9d9634b8c284",
   "metadata": {},
   "source": [
    "### Correlation Matrix (Multivariate analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70795351-50ac-41b9-b82b-e03638b68fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_matrix = data.corr(method = 'pearson',numeric_only= True).round(2)\n",
    "\n",
    "# Custom colormap\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    \"Custom\", [\"#87CEEB\", \"white\", \"#FF7F50\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize= (25,25))       \n",
    "sns.heatmap(correlation_matrix.round(2), annot= True, cmap= cmap, fmt= '.2f', linewidths= 0.3,  linecolor='black', square= True, annot_kws={\"size\": 12, \"color\": \"black\"})  \n",
    "\n",
    "plt.xticks(fontsize=18)  \n",
    "plt.yticks(fontsize=18)\n",
    "plt.title(\"Correlation Matrix CO2 Variables\", fontsize = 20)\n",
    "plt.savefig('Correlation Matrix CO2 Variables', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda6985-9b9f-4d5c-a545-99eab2861846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pearson correlation between total_emission and other features:\n",
    "total_emission_correlations = correlation_matrix['total_emission']\n",
    "print(total_emission_correlations)\n",
    "\n",
    "On_farm_electricity_use = correlation_matrix['On-farm Electricity Use']\n",
    "print(On_farm_electricity_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed979a77-b498-42a5-94ac-493152eda99a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter correlation with total_emission > 0.5 or < -0.5\n",
    "significant_correlations = total_emission_correlations[\n",
    "    (total_emission_correlations > 0.5) | (total_emission_correlations < -0.5)\n",
    "]\n",
    "selected_features = significant_correlations.index\n",
    "\n",
    "# Correlatiematrix with selected features\n",
    "reduced_correlation_matrix = correlation_matrix.loc[selected_features, selected_features]\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    \"Custom\", [\"#87CEEB\", \"white\", \"#FF7F50\"]\n",
    ")\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(14, 14))  \n",
    "sns.heatmap(\n",
    "    reduced_correlation_matrix, \n",
    "    annot=True, \n",
    "    cmap= cmap,  \n",
    "    fmt='.2f', \n",
    "    linewidths=0.3, \n",
    "    square=True, \n",
    "    annot_kws={\"size\": 10,\"color\": \"black\"},\n",
    "    center=0\n",
    ")\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title(\"Filtered Correlation Matrix\", fontsize=17)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Reduced_Correlation_Matrix_Reversed.png', dpi=300)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47edf7-eebb-4b0b-b974-7c56d6dc229c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correlation > 0.8\n",
    "filtered_correlations = total_emission_correlations[total_emission_correlations > 0.8]\n",
    "print(filtered_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603a991-e69b-430d-b970-f0dd287ae301",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4b797-2fb6-4f65-8d88-73b6f2492fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Missing values within the dataset:\n",
    "N_missingvalues = data.isnull().sum()\n",
    "R_missingvalues = data.isnull().mean() * 100\n",
    "\n",
    "# Which specific features has missing values?\n",
    "R_missingvalues = R_missingvalues[R_missingvalues > 0]\n",
    "\n",
    "print(R_missingvalues)\n",
    "\n",
    "# Plot % of missing values\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(R_missingvalues.index, R_missingvalues.values, color='skyblue')\n",
    "plt.axhline(y=10, color='r', linestyle='--', label='10% Threshold')\n",
    "plt.ylabel('Percentage Missing Values (%)')\n",
    "plt.title('Missing Values Percentage by Feature')\n",
    "plt.xticks(rotation= 90, fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout() \n",
    "plt.savefig('missingvalues_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708288d-5b4b-4e42-98e4-7da009c9b984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MCAR\n",
    "# Calculate observed missing patterns\n",
    "missing_patterns = data.isnull().astype(int)\n",
    "pattern_counts = missing_patterns.value_counts()\n",
    "\n",
    "# Create expected frequencies assuming MCAR\n",
    "expected_counts = np.full_like(pattern_counts, fill_value=np.mean(pattern_counts))\n",
    "\n",
    "# Chi-square statistic\n",
    "chi_square_stat = np.sum((pattern_counts - expected_counts)**2 / expected_counts)\n",
    "\n",
    "# Degrees of freedom\n",
    "df = len(pattern_counts) - 1\n",
    "\n",
    "# P-value\n",
    "p_value = chi2.sf(chi_square_stat, df)\n",
    "\n",
    "# Results\n",
    "print(f\"Chi-square Statistic: {chi_square_stat}, p-value: {p_value}\")\n",
    "if p_value > 0.05:\n",
    "    print(\"Data is MCAR (Missing Completely At Random).\")\n",
    "else:\n",
    "    print(\"Data is not MCAR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cdbac-fb2a-47cd-bf30-17e8c1ff6f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAR\n",
    "# Calculate correlation matrix for missingness\n",
    "plt.figure(figsize=(16, 18)) \n",
    "mask = correlation_matrix.abs() < 0.3  # Mask correlations below 0.3\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    mask=mask,  \n",
    "    annot=True, \n",
    "    fmt=\".2f\",  \n",
    "    cmap='coolwarm', \n",
    "    linewidths=0.5,\n",
    "    linecolor='black',\n",
    "    cbar_kws={'label': 'Correlation'},  # Label for color bar\n",
    "    annot_kws={\"size\": 10}  \n",
    ")\n",
    "plt.title('Correlation of Missing Values', fontsize=18)\n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('missing_values_correlation_improved.png')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58a2c9-6150-49aa-8a28-b260cddee933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['missing_count'] = data.isnull().sum(axis=1)\n",
    "sns.boxplot(x='Area', y='missing_count', data=data)\n",
    "plt.title('Missing Values by Area')\n",
    "plt.savefig('missing_values_by_area.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdf2e9-9c6b-4277-ae97-6b35e7d30dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MNAR\n",
    "# Analyze rows with missing values\n",
    "missing_rows = data[data['Crop Residues'].isnull()]\n",
    "non_missing_rows = data[~data['Crop Residues'].isnull()]\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Summary of rows with missing 'Crop Residues':\")\n",
    "print(missing_rows.describe())\n",
    "\n",
    "print(\"Summary of rows without missing 'Crop Residues':\")\n",
    "print(non_missing_rows.describe())\n",
    "\n",
    "sns.boxplot(x=data['Crop Residues'].isnull(), y=data['total_emission'])\n",
    "plt.title('Comparison of Emissions for Missing vs Non-Missing Crop Residues')\n",
    "plt.savefig('comparisonmissingornonmissingcrop.png')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0865b3-5bcd-4385-b9b6-f9df6b8254f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Outliers within the dataset:\n",
    "# Plot box plots for all numeric features\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "plt.figure(figsize=(15, len(numeric_features) * 3))\n",
    "\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    plt.subplot(len(numeric_features), 1, i + 1)\n",
    "    sns.boxplot(x=data[feature], color='skyblue')\n",
    "    plt.title(f'Box Plot of {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68eb698-7837-495d-8150-3015b49af8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the Median Absolute Deviation (MAD)\n",
    "def mad(series):\n",
    "    return np.median(np.abs(series - np.median(series)))\n",
    "\n",
    "# Calculate MAD for all numeric features\n",
    "median = data[numeric_features].median()\n",
    "mad_values = data[numeric_features].apply(mad)\n",
    "\n",
    "# Calculate MAD scores for each value\n",
    "mad_scores = data[numeric_features].sub(median).abs().div(mad_values)\n",
    "\n",
    "# Identify rows where any feature has a MAD score above a threshold (e.g., > 3)\n",
    "outliers = (mad_scores > 3).any(axis=1)\n",
    "outliers_data = data[outliers]\n",
    "\n",
    "# Count outliers per feature (MAD threshold > 3)\n",
    "outlier_counts = (mad_scores > 3).sum()\n",
    "\n",
    "# Count percentage per feature\n",
    "outlier_percentages = (outlier_counts / len(data)) * 100\n",
    "\n",
    "# Dataframe with outlier counts and percentages\n",
    "outlier_stats_df = pd.DataFrame({\n",
    "    'Feature': outlier_counts.index,\n",
    "    'Outlier_Count': outlier_counts.values,\n",
    "    'Outlier_Percentage': outlier_percentages.values\n",
    "})\n",
    "\n",
    "# Features with outliers\n",
    "outlier_stats_df = outlier_stats_df[outlier_stats_df['Outlier_Count'] > 0]\n",
    "\n",
    "print(outlier_stats_df)\n",
    "\n",
    "# Drained organic soils outlier? --> according to literature range till 100+ MtCO₂ per year\n",
    "\n",
    "max_drained= data[\"Drained organic soils (CO2)\"].max()\n",
    "upper_limit = np.percentile(data[\"Drained organic soils (CO2)\"], 99)\n",
    "print(\"max drained =\",  max_drained)\n",
    "print(\"Upper limit of winsorizing =\",  upper_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318dd85f-7443-4484-8e6c-cdcf8fecb492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot % of outliers\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(outlier_stats_df['Feature'], outlier_stats_df['Outlier_Percentage'], color='skyblue')\n",
    "plt.axhline(y=1, color='y', linestyle='--', label='1% Threshold')\n",
    "plt.axhline(y=5, color='r', linestyle='--', label='5% Threshold')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Outlier Percentage (%)', fontsize=14)\n",
    "plt.title('Outlier Percentage by Feature', fontsize=15)\n",
    "\n",
    "# Adjust x-tick labels\n",
    "plt.xticks(rotation=90, fontsize=14)\n",
    "plt.yticks(fontsize=14) \n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()  \n",
    "plt.savefig('outlier_plot2.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15223b-3c4f-4ec1-b6c3-601ef50f4e90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outlier_summary(data):\n",
    "    summary = {}\n",
    "\n",
    "    # Select numeric columns\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "    for column in numeric_data.columns:\n",
    "        max_value = numeric_data[column].max()\n",
    "        upper_limit = np.percentile(numeric_data[column], 99)  # 99 percentil as with winsorizing \n",
    "        outlier_count = (numeric_data[column] > upper_limit).sum()  \n",
    "\n",
    "        summary[column] = {\n",
    "            'Max Value': max_value,\n",
    "            '99th Percentile': upper_limit,\n",
    "            'Outlier_Count': outlier_count\n",
    "        }\n",
    "\n",
    "    # Display the information in a dataframe\n",
    "    outlier_stats_df = pd.DataFrame(summary).T\n",
    "    outlier_stats_df = outlier_stats_df[outlier_stats_df['Outlier_Count'] > 0]\n",
    "\n",
    "    return outlier_stats_df\n",
    "\n",
    "# Print the information\n",
    "outlier_info = outlier_summary(data)\n",
    "print(outlier_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29759201-f92e-40fe-b80c-fb73ff018d59",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c924ad6-1f60-470f-b879-a1a99980b202",
   "metadata": {},
   "source": [
    "### Handeling missing values & outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6aa7f-85c8-4308-98c4-cf3af6498c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a copy of the data\n",
    "data_2 = data.copy()\n",
    "\n",
    "# Handling missing values based on percentage\n",
    "# 1. Mean/Median Imputation for missing values <= 10%\n",
    "low_missing_features = R_missingvalues[R_missingvalues <= 10].index\n",
    "\n",
    "for feature in low_missing_features:\n",
    "    if feature in outlier_stats_df[outlier_stats_df['Outlier_Percentage'] > 1]['Feature'].values:\n",
    "        # Median Imputation for skewed data or >1% outliers\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "    else:\n",
    "        # Mean Imputation for non-skewed data\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "    data_2[feature] = imputer.fit_transform(data_2[[feature]])\n",
    "\n",
    "# 2. MICE with PMM for missing values > 10%\n",
    "high_missing_features = R_missingvalues[R_missingvalues > 10].index\n",
    "\n",
    "# Applying Multiple Imputation by Chained Equations (MICE) using Predictive Mean Matching (PMM)\n",
    "mice_imputer = IterativeImputer(max_iter=10, random_state=0, sample_posterior=True)\n",
    "data_2[high_missing_features] = mice_imputer.fit_transform(data_2[high_missing_features])\n",
    "\n",
    "# Handling Outliers with Winsorizing\n",
    "# Apply winsorizing for features with more than 1% outliers\n",
    "for feature in outlier_stats_df[outlier_stats_df['Outlier_Percentage'] > 1]['Feature']:\n",
    "    data_2[feature] = winsorize(data_2[feature], limits=[0.01, 0.01])\n",
    "\n",
    "# Data Check\n",
    "data_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ca9d2-ad34-444b-8291-2be1e80397d1",
   "metadata": {},
   "source": [
    "### Encoding categorical variables, diving deeper in the feature 'Area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84257b6-11d7-49cf-9b92-9e4ee26f4bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoding categorical variables? --> Area\n",
    "# Creating a copy of data_2 to work on \n",
    "data_3 = data_2.copy()\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "data_3['Area'] = label_encoder.fit_transform(data_3['Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467d5a5-6be8-46f4-869e-0892613cb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_area_distribution(data):\n",
    "    # Count the occurrences of each category in the 'Area' feature\n",
    "    distribution_area = data['Area'].value_counts()\n",
    "\n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    distribution_area.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribution of Observations by Area', fontsize=16)\n",
    "    plt.xlabel('Area', fontsize=14)\n",
    "    plt.ylabel('Number of Observations', fontsize=14)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "plot_area_distribution(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae6cd8-5b06-4008-ac75-99d263e7303d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "area_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "# Group by area\n",
    "average_emission_per_country = data_3.groupby('Area')['total_emission'].mean().reset_index()\n",
    "\n",
    "# Add original names\n",
    "average_emission_per_country['Area_name'] = average_emission_per_country['Area'].map(area_mapping)\n",
    "\n",
    "# Sort on average emission\n",
    "average_emission_per_country_sorted = average_emission_per_country.sort_values(by='total_emission', ascending=False)\n",
    "\n",
    "# Plot the top 10 countries with the highest average emission\n",
    "top_50_countries = average_emission_per_country_sorted.head(50)\n",
    "plt.figure(figsize=(10, 14))\n",
    "plt.barh(top_50_countries['Area_name'], top_50_countries['total_emission'], color='#FF7F50')\n",
    "plt.title('Top 50 Countries with Highest CO2 Emissions', fontsize=14)\n",
    "plt.xlabel('Average Total Emissions (kt)', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig(\"Top_50_CO2_Emissions.png\")\n",
    "plt.show()\n",
    "\n",
    "# Filter - emissions \n",
    "average_emission_per_country_sorted_filtered = average_emission_per_country_sorted[\n",
    "    average_emission_per_country_sorted['total_emission'] >= 0\n",
    "]\n",
    "\n",
    "# Bottom 50 countries\n",
    "bottom_50_countries_filtered = average_emission_per_country_sorted_filtered.tail(50)\n",
    "\n",
    "# Plot for 50 bottom countries\n",
    "plt.figure(figsize=(10, 14))\n",
    "plt.barh(bottom_50_countries_filtered['Area_name'], bottom_50_countries_filtered['total_emission'], color='skyblue')\n",
    "plt.title('Top 50 Countries with Lowest CO2 Emissions', fontsize=14)\n",
    "plt.xlabel('Average Total Emissions (kt)', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Bottom_50_CO2_Emissions_Filtered.png\")  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c263c-7982-43b2-a08e-0e531cecb0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate to smaller words for visualization purposes Overleaf\n",
    "def create_custom_abbreviation(name):\n",
    "    words = name.replace('(', '').replace(')', '').split()  \n",
    "    if len(words) > 1:\n",
    "        abbreviations = [word[0].upper() + '.' for word in words[1:]]\n",
    "        return words[0] + ' ' + ' '.join(abbreviations)\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "# List of countries\n",
    "top_25_countries = top_25_countries.copy()  \n",
    "bottom_25_countries_filtered = bottom_25_countries_filtered.copy()  \n",
    "\n",
    "top_25_countries['Area_abbreviation'] = top_25_countries['Area_name'].apply(create_custom_abbreviation)\n",
    "bottom_25_countries_filtered['Area_abbreviation'] = bottom_25_countries_filtered['Area_name'].apply(create_custom_abbreviation)\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 16))\n",
    "\n",
    "# Plot for the top 25 countries\n",
    "axes[0].barh(top_25_countries['Area_abbreviation'], top_25_countries['total_emission'], color='#FF7F50')\n",
    "axes[0].set_title('Top 25 Countries with Highest CO2 Emissions', fontsize=25)\n",
    "axes[0].set_xlabel('Average Total Emissions (kt)', fontsize=25)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].tick_params(axis='y', labelsize=24)  \n",
    "axes[0].tick_params(axis='x', labelsize=24) \n",
    "\n",
    "# Plot for the bottom 25 countries\n",
    "axes[1].barh(bottom_25_countries_filtered['Area_abbreviation'], bottom_25_countries_filtered['total_emission'], color='skyblue')\n",
    "axes[1].set_title('Top 25 Countries with Lowest CO2 Emissions', fontsize=25)\n",
    "axes[1].set_xlabel('Average Total Emissions (kt)', fontsize=25)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].tick_params(axis='y', labelsize=24)  \n",
    "axes[1].tick_params(axis='x', labelsize=24)  \n",
    "\n",
    "# Larger text on the axes\n",
    "for ax in axes:\n",
    "    ax.xaxis.get_offset_text().set_fontsize(24)  \n",
    "\n",
    "# Make the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Top_and_Bottom_25_CO2_Emissions_Custom_Abbreviations.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a83ee-8d67-4a2c-8cb6-b08fb56987a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pearson correlation of Area with other features\n",
    "area_correlations = data_3.corr(method='pearson', numeric_only=True)['Area'].round(2)\n",
    "\n",
    "# Remove the correlation with 'Area' itselfs\n",
    "area_correlations_without_self = area_correlations.drop('Area')\n",
    "\n",
    "# Sort correlations\n",
    "top_positive_area_correlations = area_correlations_without_self.sort_values(ascending=False).head(10)\n",
    "top_negative_area_correlations = area_correlations_without_self.sort_values(ascending=True).head(10)\n",
    "\n",
    "# print results \n",
    "print(\"Top 10 positive correlated features with 'Area':\")\n",
    "print(top_positive_area_correlations)\n",
    "\n",
    "print(\"\\nTop 10 negative correlated features with 'Area':\")\n",
    "print(top_negative_area_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e0514-595f-4f51-a472-87f940a54a31",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bacf3-8832-4454-ad87-ab669eaf72f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scaling with Standardization\n",
    "data_4 = data_3.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_features = data_4.select_dtypes(include=['int64', 'float64']).columns\n",
    "data_4[numerical_features] = scaler.fit_transform(data_4[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083310b7-5897-4fbd-bac0-498d3ffc5d22",
   "metadata": {},
   "source": [
    "## 3. Model Development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7dda9-3fad-466b-9152-f3ecbc9b98f8",
   "metadata": {},
   "source": [
    "### (initial) Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf7beb-c761-4870-b649-bd6d480eac23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = data_4.drop(columns= ['total_emission'])\n",
    "y = data_4['total_emission']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b891e-e69e-4f25-a4cc-99d40d768e94",
   "metadata": {},
   "source": [
    "### Spliting data in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df5da6-70fb-47df-b006-9cde9740aff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the data in training and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0af787-3e35-44c4-a922-c03dac6cf8fd",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cce5b6-8733-403f-972c-7e77c638a44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e637d79-46aa-4bce-a15d-27b8b05faeb0",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170cd5c-def8-4409-a243-2caec8e36927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR_model = LinearRegression()\n",
    "\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Apply cross-validation in loop\n",
    "for train_index, val_index in kf.split(x):\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    LR_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict y on the validation fold\n",
    "    y_pred = LR_model.predict(x_val)\n",
    "    \n",
    "    # Evaluate \n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Add results to the lists\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "# Print average scores and standard deviations \n",
    "print(\"Average MSE:\", np.mean(mse_list), \"Standard deviation MSE:\", np.std(mse_list))\n",
    "print(\"Average MAE:\", np.mean(mae_list), \"Standard deviation MAE:\", np.std(mae_list))\n",
    "print(\"Average R²:\", np.mean(r2_list), \"Standard deviation R²:\", np.std(r2_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851723ee-c2c2-40d8-98bd-ae0da67a1acf",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47beabcf-0cd2-46a3-a5eb-dbd21a5274eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RF_model = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=0)\n",
    "\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, val_index in kf.split(x):\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    RF_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict y on the validation fold\n",
    "    y_pred = RF_model.predict(x_val)\n",
    "   \n",
    "    # Evaluate \n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Add results to the lists\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "    \n",
    "# Print metrics as baseline evaluation, not as final model evaluation\n",
    "print(\"Baseline evaluation:\")\n",
    "print(\"Average MSE:\", np.mean(mse_list), \"Standard deviation MSE:\", np.std(mse_list))\n",
    "print(\"Average MAE:\", np.mean(mae_list), \"Standard deviation MAE:\", np.std(mae_list))\n",
    "print(\"Average R²:\", np.mean(r2_list), \"Standard deviation R²:\", np.std(r2_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f51e2-bedb-4ae5-af9f-7eafec2a007f",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning RF (remove # for running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460aa6e3-6848-4d8f-a23e-f4df0887bf66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # RF model without parameters\n",
    "# RF_model_blanco =  RandomForestRegressor(random_state=0)\n",
    "\n",
    "# # Hyperparameter tuning with BayesSearchCV \n",
    "# search_spaces = {\n",
    "#     'n_estimators': (50, 150),\n",
    "#     'max_depth': (5, 50),\n",
    "#     'min_samples_split': (2, 10),\n",
    "#     'min_samples_leaf': (1, 10)\n",
    "# }\n",
    "\n",
    "# # BayesSearchCV \n",
    "# bayes_search = BayesSearchCV(\n",
    "#     estimator=RF_model_blanco,\n",
    "#     search_spaces=search_spaces,\n",
    "#     n_iter=100,  \n",
    "#     cv=5,  \n",
    "#     n_jobs=-1,   \n",
    "#     random_state=0\n",
    "# )\n",
    "\n",
    "# # BayesSearchCV fit\n",
    "# tuned_model = bayes_search.fit(x, y)\n",
    "\n",
    "# best_params = bayes_search.best_params_\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"The best Hyperparameters of RF:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42a716-54e8-40aa-a64d-9f2264dbc318",
   "metadata": {},
   "source": [
    "#### RF Model with Hyperparmeter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82fc998-e485-4298-b8d9-8a548828e62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for RF_tuned (Output previous cell with randomstate = 0)\n",
    "best_params = {\n",
    "    \"n_estimators\": 51,\n",
    "    \"max_depth\": 50,\n",
    "    \"min_samples_split\": 3,\n",
    "    \"min_samples_leaf\": 1\n",
    "}\n",
    "\n",
    "# RF model\n",
    "RF_tuned = RandomForestRegressor(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Cross-validation loop\n",
    "mse_list_tuned = []\n",
    "mae_list_tuned = []\n",
    "r2_list_tuned = []\n",
    "\n",
    "for train_index, val_index in kf.split(x):\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    #Train the model\n",
    "    RF_tuned.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict y on the validation fold\n",
    "    y_pred = RF_tuned.predict(x_val)\n",
    "   \n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Add results to the lists\n",
    "    mse_list_tuned.append(mse)\n",
    "    mae_list_tuned.append(mae)\n",
    "    r2_list_tuned.append(r2)\n",
    "\n",
    "# Print evaluation\n",
    "print(\"\\nModel with Hyperparameter Tuning:\")\n",
    "print(\"Gemiddelde MSE:\", np.mean(mse_list_tuned), \"Standaarddeviatie MSE:\", np.std(mse_list_tuned))\n",
    "print(\"Gemiddelde MAE:\", np.mean(mae_list_tuned), \"Standaarddeviatie MAE:\", np.std(mae_list_tuned))\n",
    "print(\"Gemiddelde R²:\", np.mean(r2_list_tuned), \"Standaarddeviatie R²:\", np.std(r2_list_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9dac1-2f34-4daf-b7a2-1a7c3e4f133c",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b27ae-282e-4f25-9096-f5d227edc343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GB_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
    "\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, val_index in kf.split(x):\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    GB_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict y on the validation fold\n",
    "    y_pred = GB_model.predict(x_val)\n",
    "   \n",
    "    # Evaluate \n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Add results to the lists\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "# Print metrics as insights, not as final model evaluation\n",
    "print(\"Baseline evaluation:\")\n",
    "print(\"Average MSE:\", np.mean(mse_list), \"Standard deviation MSE:\", np.std(mse_list))\n",
    "print(\"Average MAE:\", np.mean(mae_list), \"Standard deviation MAE:\", np.std(mae_list))\n",
    "print(\"Average R²:\", np.mean(r2_list), \"Standard deviation R²:\", np.std(r2_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9ef2c-e5a8-4cdf-8eeb-8dd285240875",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning GBM (Remove # for running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a968a-8690-4118-a58f-b45c17e397f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Baseline Gradient Boosting Regressor\n",
    "# GB_model_blanco = GradientBoostingRegressor(random_state=0, n_iter_no_change=10,\n",
    "#     validation_fraction=0.1)\n",
    "\n",
    "# # Hyperparameter tuning with BayesSearchCV \n",
    "# search_spaces = {\n",
    "#     'n_estimators': (50, 500),\n",
    "#     'learning_rate': (0.01, 0.2, 'log-uniform'),\n",
    "#     'max_depth': (3, 15),\n",
    "#     'min_samples_split': (2, 25),\n",
    "#     'min_samples_leaf': (1, 40)\n",
    "# }\n",
    "\n",
    "# # BayesSearchCV \n",
    "# bayes_search = BayesSearchCV(\n",
    "#     estimator=GB_model_blanco,\n",
    "#     search_spaces=search_spaces,\n",
    "#     n_iter=80,  \n",
    "#     cv=5,  \n",
    "#     n_jobs=-1,   \n",
    "#     random_state=0\n",
    "# )\n",
    "\n",
    "# # Fit BayesSearchCV to the data\n",
    "# tuned_modelGBM = bayes_search.fit(x, y)\n",
    "\n",
    "# best_paramsGBM = tuned_modelGBM.best_params_\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"The best Hyperparameters of GBM:\", best_paramsGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b2d43-8c57-4992-a0e9-d552a3f9e590",
   "metadata": {},
   "source": [
    "#### GBM model with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ad2c4-bb6d-4205-9cfa-2ba5b4e5c55e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for GBM_tuned (Output previous cell with randomstate = 0)\n",
    "best_paramsGBM = {\n",
    "    \"learning_rate\": 0.08107087827070233,\n",
    "    \"max_depth\": 11,\n",
    "    \"min_samples_leaf\": 40,\n",
    "    \"min_samples_split\": 25,\n",
    "    \"n_estimators\": 488\n",
    "}\n",
    "\n",
    "# Train GBM with the hyperparameters\n",
    "GBM_tuned = GradientBoostingRegressor(\n",
    "    n_estimators=best_paramsGBM['n_estimators'],\n",
    "    max_depth=best_paramsGBM['max_depth'],\n",
    "    learning_rate=best_paramsGBM['learning_rate'],\n",
    "    min_samples_split=best_paramsGBM['min_samples_split'],\n",
    "    min_samples_leaf=best_paramsGBM['min_samples_leaf'],\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Cross-validation evaluation\n",
    "mse_list_gbm, mae_list_gbm, r2_list_gbm = [], [], []\n",
    "\n",
    "for train_index, val_index in kf.split(x):\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    GBM_tuned.fit(x_train, y_train)\n",
    "    y_pred = GBM_tuned.predict(x_val)\n",
    "    \n",
    "    mse_list_gbm.append(mean_squared_error(y_val, y_pred))\n",
    "    mae_list_gbm.append(mean_absolute_error(y_val, y_pred))\n",
    "    r2_list_gbm.append(r2_score(y_val, y_pred))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nGBM Model with Hyperparameter Tuning:\")\n",
    "print(\"Average MSE:\", np.mean(mse_list_gbm), \"Standaarddeviatie MSE:\", np.std(mse_list_gbm))\n",
    "print(\"Average MAE:\", np.mean(mae_list_gbm), \"Standaarddeviatie MAE:\", np.std(mae_list_gbm))\n",
    "print(\"Average R²:\", np.mean(r2_list_gbm), \"Standaarddeviatie R²:\", np.std(r2_list_gbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59b277-b95d-436f-9bf1-8e56378347c3",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc22f0-eb7b-4cab-aeff-4a6a4ff20bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse_list = []\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "\n",
    "FNN_model = MLPRegressor(hidden_layer_sizes=(1000,), activation='relu', solver='adam', \n",
    "                         max_iter=1000, random_state=0)\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(x):\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Training the model\n",
    "    FNN_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Made predictions\n",
    "    y_pred = FNN_model.predict(x_test)\n",
    "   \n",
    "    # Evaluation\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Add results to the list\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Baseline evaluation:\")\n",
    "print(\"Average MSE:\", np.mean(mse_list), \"Standard deviation MSE:\", np.std(mse_list))\n",
    "print(\"Average MAE:\", np.mean(mae_list), \"Standard deviation MAE:\", np.std(mae_list))\n",
    "print(\"Average R²:\", np.mean(r2_list), \"Standard deviation R²:\", np.std(r2_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33bbeb-9093-4773-bb19-c31eb163109d",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning FNN (remove # for running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc77dc-a9e4-4b9e-81b3-66c5fc192073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(type(x), type(y))\n",
    "# print(x.shape, y.shape)\n",
    "\n",
    "# # Define the baseline MLPRegressor model\n",
    "# FNN_model_blanco = MLPRegressor(max_iter=2000, random_state=0, early_stopping=True)\n",
    "\n",
    "# # Define the parameter search space\n",
    "# search_spaces = {\n",
    "#     'hidden_layer_sizes': Categorical([50, 100]),\n",
    "#     'activation': Categorical(['relu', 'tanh']),\n",
    "#     'solver': Categorical(['adam', 'lbfgs']),\n",
    "#     'alpha': Real(1e-4, 5e-4, prior='log-uniform'),\n",
    "#     'learning_rate_init': Real(1e-4, 1e-2, prior='log-uniform')\n",
    "# }\n",
    "\n",
    "# # BayesSearchCV for hyperparameter tuning\n",
    "# bayes_search = BayesSearchCV(\n",
    "#     estimator=FNN_model_blanco,\n",
    "#     search_spaces=search_spaces,\n",
    "#     n_iter=100,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=0,\n",
    "#     verbose=3  # Enable logging to trace the process\n",
    "# )\n",
    "\n",
    "# # Perform hyperparameter tuning with a callback\n",
    "# tuned_modelFNN = bayes_search.fit(x, y, callback=[DeltaYStopper(1e-6)])\n",
    "\n",
    "# # Get the best parameters found\n",
    "# best_paramsFNN = tuned_modelFNN.best_params_\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"The best Hyperparameters:\", best_paramsFNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f3db6-53f4-4fee-9895-8dfeea51be9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### FNN with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298bab7-bf79-4446-853c-7fca35fe5595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for GBM_tuned (Output previous cell with randomstate = 0)\n",
    "best_paramsFNN = {\n",
    "    \"hidden_layer_sizes\": 100,\n",
    "    \"activation\": \"relu\",\n",
    "    \"solver\": \"adam\",\n",
    "    \"alpha\": 0.0001,\n",
    "    \"learning_rate_init\": 0.0017735704525922659\n",
    "}\n",
    "\n",
    "    \n",
    "# Train FNN with hyperparameters\n",
    "FNN_tuned = MLPRegressor(\n",
    "    hidden_layer_sizes=best_paramsFNN['hidden_layer_sizes'],\n",
    "    activation=best_paramsFNN['activation'],\n",
    "    solver=best_paramsFNN['solver'],\n",
    "    alpha=best_paramsFNN['alpha'],  \n",
    "    learning_rate= 'adaptive',  \n",
    "    learning_rate_init=best_paramsFNN['learning_rate_init'],  \n",
    "    max_iter=5000, # was 2000\n",
    "    early_stopping=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Cross-validation evaluation\n",
    "mse_list_fnn, mae_list_fnn, r2_list_fnn = [], [], []\n",
    "\n",
    "for train_index, val_index in kf.split(x):\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    FNN_tuned.fit(x_train, y_train)\n",
    "    y_pred = FNN_tuned.predict(x_val)\n",
    "    \n",
    "    mse_list_fnn.append(mean_squared_error(y_val, y_pred))\n",
    "    mae_list_fnn.append(mean_absolute_error(y_val, y_pred))\n",
    "    r2_list_fnn.append(r2_score(y_val, y_pred))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFNN Model with Hyperparameter Tuning:\")\n",
    "print(\"Mean MSE:\", np.mean(mse_list_fnn), \"Standaarddeviatie MSE:\", np.std(mse_list_fnn))\n",
    "print(\"Mean MAE:\", np.mean(mae_list_fnn), \"Standaarddeviatie MAE:\", np.std(mae_list_fnn))\n",
    "print(\"Mean R²:\", np.mean(r2_list_fnn), \"Standaarddeviatie R²:\", np.std(r2_list_fnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc415cd-9d6d-4750-b4be-8b41fb7ff523",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06050d44-f9b0-436e-b717-4b57f5122f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add Gaussian perturbation to features\n",
    "def perturb_features(X, features_to_perturb, std_dev=0.1):\n",
    "    np.random.seed(0)\n",
    "    X_perturbed = X.copy()\n",
    "    for feature in features_to_perturb:\n",
    "        if feature not in X.columns:\n",
    "            raise ValueError(f\"Feature '{feature}' not found in DataFrame.\")\n",
    "        X_perturbed[feature] += np.random.normal(0, std_dev, size=X.shape[0])\n",
    "    return X_perturbed\n",
    "\n",
    "\n",
    "def compute_pgi2(model, X, y, feature_subset, std_dev=0.1):\n",
    "    \"\"\"\n",
    "    Compute the PGI² metric for a given feature subset.\n",
    "    Ensures compatibility with models expecting feature names.\n",
    "    \"\"\"\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise ValueError(f\"Input X must be a pandas DataFrame, but got {type(X)} instead.\")\n",
    "\n",
    "    # Original predictions\n",
    "    original_preds = model.predict(X)\n",
    "\n",
    "    # Perturb features and maintain DataFrame structure\n",
    "    perturbed_X = perturb_features(X, feature_subset, std_dev)\n",
    "\n",
    "    if not perturbed_X.columns.equals(X.columns):\n",
    "        raise ValueError(\"Column names of perturbed_X do not match original X.\")\n",
    "\n",
    "    # Predictions on perturbed data\n",
    "    perturbed_preds = model.predict(perturbed_X)\n",
    "\n",
    "    # Compute and return PGI²\n",
    "    return np.mean((perturbed_preds - original_preds) ** 2)\n",
    "\n",
    "\n",
    "# Compute average PGI2 for a feature ranking\n",
    "def compute_average_pgi2(model, X, y, feature_ranking, std_dev=0.1):\n",
    "    \"\"\"\n",
    "    Compute the average PGI² metric for a given feature ranking.\n",
    "    \"\"\"\n",
    "    d = len(feature_ranking)  # Total number of features\n",
    "    total_pgi2 = 0.0\n",
    "\n",
    "    for k in range(1, d + 1):  # Iterate over subset sizes\n",
    "        subset = feature_ranking[:k]\n",
    "        pgi2 = compute_pgi2(model, X, y, subset, std_dev)\n",
    "        total_pgi2 += pgi2\n",
    "\n",
    "    average_pgi2 = total_pgi2 / d\n",
    "    return average_pgi2\n",
    "\n",
    "\n",
    "# Rank features using PGI2 with greedy selection\n",
    "def rank_features_pgi2(model, X, y, std_dev=0.1):\n",
    "    \"\"\"\n",
    "    Rank features based on PGI² using a greedy selection algorithm.\n",
    "    \"\"\"\n",
    "    feature_indices = list(range(X.shape[1]))\n",
    "    feature_names = X.columns.tolist()  # Map indices to column names\n",
    "    ranked_features = []\n",
    "    remaining_features = set(feature_indices)\n",
    "\n",
    "    while remaining_features:\n",
    "        best_feature = None\n",
    "        best_pgi2 = -np.inf\n",
    "\n",
    "        for feature in remaining_features:\n",
    "            current_subset = ranked_features + [feature_names[feature]]  # Map index to name\n",
    "            pgi2 = compute_pgi2(model, X, y, current_subset, std_dev)\n",
    "            if pgi2 > best_pgi2:\n",
    "                best_pgi2 = pgi2\n",
    "                best_feature = feature\n",
    "\n",
    "        ranked_features.append(feature_names[best_feature])  # Append the feature name\n",
    "        remaining_features.remove(best_feature)\n",
    "\n",
    "    return ranked_features\n",
    "\n",
    "print(\"Feature Importance Analysis with PGI2:\")\n",
    "\n",
    "# Ensure X is a DataFrame\n",
    "if not isinstance(x_train, pd.DataFrame):\n",
    "    raise ValueError(\"Input x_train must be a pandas DataFrame with valid feature names.\")\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "rf_ranked_features = rank_features_pgi2(RF_tuned, x_train, y_train)\n",
    "print(f\"Random Forest ranked features: {rf_ranked_features}\")\n",
    "\n",
    "# Compute average PGI² for RF model\n",
    "rf_average_pgi2 = compute_average_pgi2(RF_tuned, x_train, y_train, rf_ranked_features)\n",
    "print(f\"Random Forest Average PGI²: {rf_average_pgi2}\")\n",
    "\n",
    "# Gradient Boosting Feature Importance\n",
    "gb_ranked_features = rank_features_pgi2(GBM_tuned, x_train, y_train)\n",
    "print(f\"Gradient Boosting ranked features: {gb_ranked_features}\")\n",
    "\n",
    "# Compute average PGI² for GB model\n",
    "gb_average_pgi2 = compute_average_pgi2(GBM_tuned, x_train, y_train, gb_ranked_features)\n",
    "print(f\"Gradient Boosting Average PGI²: {gb_average_pgi2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e75bc-41c3-4ce2-8778-6f81657ccfce",
   "metadata": {},
   "source": [
    "### Ploting the feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1973a95e-ae04-41bf-b372-e280039e25e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PGI² scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072d96b-62a0-416b-b6fe-ba02d5a42e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank features and calculate their PGI² scores\n",
    "def rank_features_with_scores(model, X, y, std_dev=0.1):\n",
    "    \"\"\"\n",
    "    Rank features based on PGI² and return their scores.\n",
    "    \"\"\"\n",
    "    feature_indices = list(range(X.shape[1]))\n",
    "    feature_names = X.columns.tolist()  # Map indices to column names\n",
    "    scores = {name: 0 for name in feature_names}\n",
    "    ranked_features = []\n",
    "    remaining_features = set(feature_indices)\n",
    "\n",
    "    while remaining_features:\n",
    "        best_feature = None\n",
    "        best_pgi2 = -np.inf\n",
    "\n",
    "        for feature in remaining_features:\n",
    "            current_subset = ranked_features + [feature_names[feature]]  # Map index to name\n",
    "            pgi2 = compute_pgi2(model, X, y, current_subset, std_dev)\n",
    "            if pgi2 > best_pgi2:\n",
    "                best_pgi2 = pgi2\n",
    "                best_feature = feature\n",
    "\n",
    "        ranked_features.append(feature_names[best_feature])  # Append the feature name\n",
    "        scores[feature_names[best_feature]] = best_pgi2  # Save the PGI² score\n",
    "        remaining_features.remove(best_feature)\n",
    "\n",
    "    return ranked_features, scores\n",
    "\n",
    "# Combine scores from both models for comparison\n",
    "def combine_model_scores(rf_scores, gb_scores):\n",
    "    \"\"\"\n",
    "    Combine feature scores from RF and GBM into a single DataFrame.\n",
    "    \"\"\"\n",
    "    combined_scores = pd.DataFrame({\n",
    "        \"Feature\": list(rf_scores.keys()),\n",
    "        \"RF_PGI² Score\": list(rf_scores.values()),\n",
    "        \"GBM_PGI² Score\": list(gb_scores.values())\n",
    "    })\n",
    "    combined_scores = combined_scores.sort_values(by=\"RF_PGI² Score\", ascending=False)\n",
    "    return combined_scores\n",
    "\n",
    "# Rank features and calculate their PGI² scores for both models\n",
    "rf_ranked_features, rf_scores = rank_features_with_scores(RF_tuned, x_train, y_train)\n",
    "gb_ranked_features, gbm_scores = rank_features_with_scores(GBM_tuned, x_train, y_train)\n",
    "\n",
    "# Combine scores for both models\n",
    "combined_scores = combine_model_scores(rf_scores, gbm_scores)\n",
    "\n",
    "# Adjust text size for the plot\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Plot combined feature importances\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "width = 0.4  # Width of the bars\n",
    "indices = np.arange(len(combined_scores))  # Bar positions\n",
    "\n",
    "# Plot RF scores\n",
    "ax.barh(indices - width/2, combined_scores[\"RF_PGI² Score\"], height=width, label=\"Random Forest\", color='skyblue')\n",
    "\n",
    "# Plot GBM scores\n",
    "ax.barh(indices + width/2, combined_scores[\"GBM_PGI² Score\"], height=width, label=\"Gradient Boosting Machines\", color='#FF7F50')\n",
    "\n",
    "# Add labels and legend with larger font sizes\n",
    "ax.set_yticks(indices)\n",
    "ax.set_yticklabels(combined_scores[\"Feature\"], fontsize=15)  \n",
    "ax.set_xlabel(\"PGI² Score\", fontsize=15) \n",
    "ax.set_title(\"Feature Importance Comparison: RF vs GBM\", fontsize=16)  \n",
    "ax.legend(fontsize=12) \n",
    "\n",
    "plt.gca().invert_yaxis()  \n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_comparisonRFvsGBM.png')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee38a1e-aa50-40b1-a2d9-d968df01683f",
   "metadata": {},
   "source": [
    "#### Highest rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552513c-d93f-4888-9aa6-04d01a34e62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine feature rankings from both models\n",
    "def combine_model_rankings_clean(rf_scores, gb_scores):\n",
    "    \"\"\"\n",
    "    Combine feature rankings from RF and GBM into a single DataFrame with highest ranking.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with rankings only\n",
    "    combined_rankings = pd.DataFrame({\n",
    "        \"Feature\": list(rf_scores.keys()),  # Feature names\n",
    "        \"RF_Rank\": pd.Series(rf_scores).rank(ascending=False).astype(int),  # RF ranking\n",
    "        \"GBM_Rank\": pd.Series(gb_scores).rank(ascending=False).astype(int)  # GBM ranking\n",
    "    }).set_index(\"Feature\")  # Set Feature as index to avoid duplication\n",
    "    \n",
    "    # Sort by RF rank as default\n",
    "    combined_rankings = combined_rankings.sort_values(by=\"RF_Rank\")\n",
    "    return combined_rankings\n",
    "\n",
    "# Combine rankings for both models\n",
    "combined_rankings_clean = combine_model_rankings_clean(rf_scores, gbm_scores)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(combined_rankings_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e4955-baaa-4152-b646-f3e0b0f89576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the top 10 features with the largest ranking differences\n",
    "def largest_ranking_differences(rf_scores, gb_scores):\n",
    "    \"\"\"\n",
    "    Identify the top 10 features with the largest differences in rankings between RF and GBM.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with rankings and calculate differences\n",
    "    combined_rankings = pd.DataFrame({\n",
    "        \"Feature\": list(rf_scores.keys()),  # Feature names\n",
    "        \"RF_Rank\": pd.Series(rf_scores).rank(ascending=False).astype(int),  # RF ranking\n",
    "        \"GBM_Rank\": pd.Series(gb_scores).rank(ascending=False).astype(int)  # GBM ranking\n",
    "    })\n",
    "    combined_rankings[\"Rank_Difference\"] = (combined_rankings[\"RF_Rank\"] - combined_rankings[\"GBM_Rank\"]).abs()\n",
    "    \n",
    "    # Sort by the largest differences and select the top 10\n",
    "    largest_differences = combined_rankings.sort_values(by=\"Rank_Difference\", ascending=False).head(10)\n",
    "    return largest_differences\n",
    "\n",
    "# Get the top 10 features with the largest ranking differences\n",
    "top_10_differences = largest_ranking_differences(rf_scores, gbm_scores)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(top_10_differences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9393d8-88f2-41f4-815f-35abcd2f413c",
   "metadata": {},
   "source": [
    "#### Lowest rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cc17e-1e4b-42bc-baac-8c08d0a771c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the lowest ranked features\n",
    "def lowest_ranking_features(rf_scores, gb_scores, top_n=10):\n",
    "    \"\"\"\n",
    "    Identify the features with the lowest combined rankings from RF and GBM.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with rankings\n",
    "    combined_rankings = pd.DataFrame({\n",
    "        \"Feature\": list(rf_scores.keys()),  # Feature names\n",
    "        \"RF_Rank\": pd.Series(rf_scores).rank(ascending=False).astype(int),  # RF ranking\n",
    "        \"GBM_Rank\": pd.Series(gb_scores).rank(ascending=False).astype(int)  # GBM ranking\n",
    "    })\n",
    "    \n",
    "    # Calculate a combined rank by summing RF and GBM ranks\n",
    "    combined_rankings[\"Combined_Rank\"] = combined_rankings[\"RF_Rank\"] + combined_rankings[\"GBM_Rank\"]\n",
    "    \n",
    "    # Sort by combined rank to get the lowest overall priority features\n",
    "    lowest_ranked = combined_rankings.sort_values(by=\"Combined_Rank\", ascending=False).head(top_n)\n",
    "    return lowest_ranked\n",
    "\n",
    "# Get the top 10 lowest ranked features\n",
    "lowest_10_ranked_features = lowest_ranking_features(rf_scores, gbm_scores)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(lowest_10_ranked_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194c994-b417-4cf7-9a1a-e205a74043c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort worst-ranked features on top\n",
    "def sort_worst_rankings(rf_scores, gb_scores):\n",
    "    \"\"\"\n",
    "    Combine feature rankings and sort by the lowest priority features.\n",
    "    \"\"\"\n",
    "    combined_rankings = pd.DataFrame({\n",
    "        \"Feature\": list(rf_scores.keys()),  # Feature names\n",
    "        \"RF_Rank\": pd.Series(rf_scores).rank(ascending=False).astype(int),  # RF ranking\n",
    "        \"GBM_Rank\": pd.Series(gb_scores).rank(ascending=False).astype(int)  # GBM ranking\n",
    "    })\n",
    "\n",
    "    # Sort by RF_Rank and GBM_Rank in descending order (worst ranks first)\n",
    "    combined_rankings[\"Max_Rank\"] = combined_rankings[[\"RF_Rank\", \"GBM_Rank\"]].max(axis=1)\n",
    "    combined_rankings = combined_rankings.sort_values(by=\"Max_Rank\", ascending=False)\n",
    "\n",
    "    return combined_rankings.drop(columns=[\"Max_Rank\"])\n",
    "\n",
    "# Get the worst-ranked features sorted\n",
    "worst_ranked_features = sort_worst_rankings(rf_scores, gbm_scores)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(worst_ranked_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcdc72c-754f-409b-885c-8617eb67417e",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932160df-9a2f-4515-bcf6-30bd5a23b5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. SHAP RF\n",
    "explainer_rf = shap.TreeExplainer(RF_tuned)\n",
    "shap_values_rf = explainer_rf.shap_values(x_train)\n",
    "\n",
    "# 2. SHAP GB\n",
    "explainer_gbm = shap.TreeExplainer(GBM_tuned)\n",
    "shap_values_gbm = explainer_gbm.shap_values(x_train)\n",
    "\n",
    "shap_importance_rf = np.abs(shap_values_rf).mean(axis=0)\n",
    "shap_importance_gbm = np.abs(shap_values_gbm).mean(axis=0)\n",
    "\n",
    "# Combine SHAP values for plot\n",
    "shap_scores = pd.DataFrame({\n",
    "    \"Feature\": x_train.columns,\n",
    "    \"RF_SHAP Score\": shap_importance_rf,\n",
    "    \"GBM_SHAP Score\": shap_importance_gbm\n",
    "}).sort_values(by=\"RF_SHAP Score\", ascending=False)\n",
    "\n",
    "# Plot SHAP values\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "width = 0.35 \n",
    "indices = np.arange(len(shap_scores))  \n",
    "\n",
    "# BRF SHAP-scores\n",
    "ax.barh(indices - width/2, shap_scores[\"RF_SHAP Score\"], height=width, label=\"Random Forest\", color='skyblue')\n",
    "\n",
    "# GBM SHAP-scores\n",
    "ax.barh(indices + width/2, shap_scores[\"GBM_SHAP Score\"], height=width, label=\"Gradient Boosting\", color='#FF7F50')\n",
    "\n",
    "# Labels\n",
    "ax.set_yticks(indices)\n",
    "ax.set_yticklabels(shap_scores[\"Feature\"])\n",
    "ax.set_xlabel(\"SHAP Score\")\n",
    "ax.set_title(\"Feature Importance Based on SHAP: RF vs GBM\")\n",
    "ax.legend()\n",
    "\n",
    "# Important Features as first:\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_comparison_RF_GBM.png')\n",
    "plt.show()\n",
    "\n",
    "# Beeswarm plot Random Forest\n",
    "shap.summary_plot(shap_values_rf, x_train)\n",
    "\n",
    "# Beeswarm plot Gradient Boosting\n",
    "shap.summary_plot(shap_values_gbm, x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243d909-d852-4321-894e-d98928742d8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Multicollineariteit PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab99370-39b8-47b7-b3a1-ea0bea35bbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA model\n",
    "# features without the target variable:\n",
    "features_2 = data_4.drop(columns=['total_emission'])\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(features_2)\n",
    "\n",
    "# Variance per component \n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance per Component:\", explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24359693-f58e-4c0c-b3fe-735c2160485f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the loadings (original variables' contributions to each component)\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(len(pca.components_))],\n",
    "    index=features_2.columns  \n",
    ")\n",
    "\n",
    "for pc in loadings.columns:\n",
    "    # Select top 5 variables\n",
    "    top_loadings = loadings[pc].abs().nlargest(5)\n",
    "    print(f\"Top 5 Variables for {pc}:\\n{top_loadings}\\n\")\n",
    "\n",
    "\n",
    "# Project data onto the principal components\n",
    "projected_data = pd.DataFrame(pca.transform(features_2), columns=[f'PC{i+1}' for i in range(features_2.shape[1])])\n",
    "\n",
    "# Scatterplot of the first two principal components \n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(projected_data['PC1'], projected_data['PC2'], alpha=0.7, color='skyblue')\n",
    "plt.xlabel(\"PC1\", fontsize=14)  \n",
    "plt.ylabel(\"PC2\", fontsize=14)  \n",
    "plt.title(\"Projection onto PC1 and PC2\", fontsize=16)  \n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.savefig(\"scatterplot_pc1_pc2_larger_text.png\", dpi=300)  \n",
    "plt.show()\n",
    "\n",
    "# Scree plot to show cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance)+1), np.cumsum(explained_variance), marker='o', linestyle='--', color='skyblue')\n",
    "plt.xlabel(\"Number of Components\", fontsize=14)  \n",
    "plt.ylabel(\"Cumulative Explained Variance\", fontsize=14) \n",
    "plt.title(\"Scree Plot\", fontsize=16) \n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)\n",
    "plt.savefig(\"screeplot_cumulative_variance_larger_text.png\", dpi=300)  \n",
    "plt.show()\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Print cumulative variance for PC1 and PC2\n",
    "pc1_pc2_variance = cumulative_variance[1]  # Cumulative variance up to PC2\n",
    "print(f\"Cumulative explained variance for PC1 and PC2: {pc1_pc2_variance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97212e-6c3b-415a-9496-722e8cf78548",
   "metadata": {},
   "source": [
    "## Feature selection 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be819541-ca5e-4d90-a62f-5010167e0d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selected features with RF PGI² > 0.015\n",
    "selected_features = [feature for feature, score in rf_scores.items() if score > 0.015]\n",
    "\n",
    "# Copy of x with the selected features only\n",
    "x_2 = x[selected_features]\n",
    "\n",
    "# Print the new features set:\n",
    "print(\"Selected Features:\", selected_features)\n",
    "\n",
    "print(len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b5b9b-56d3-4ae2-987f-f68f9acb5d26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LR Performance after Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a1933-809d-4287-9c29-487534caffd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR_model_tuned2 = LinearRegression()\n",
    "\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Apply cross-validation in loop\n",
    "for train_index, val_index in kf.split(x_2):  \n",
    "    x_train, x_val = x_2.iloc[train_index], x_2.iloc[val_index]  \n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    LR_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict y on the validation fold\n",
    "    y_pred = LR_model.predict(x_val)\n",
    "    \n",
    "    # Evaluate \n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Add results to the lists\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "# Print average scores and standard deviations \n",
    "print(\"Average MSE:\", np.mean(mse_list), \"Standard deviation MSE:\", np.std(mse_list))\n",
    "print(\"Average MAE:\", np.mean(mae_list), \"Standard deviation MAE:\", np.std(mae_list))\n",
    "print(\"Average R²:\", np.mean(r2_list), \"Standard deviation R²:\", np.std(r2_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fbfff-8f2f-477a-bd18-1f105cd4a4b3",
   "metadata": {},
   "source": [
    "### RF Performance after Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd66c05-6067-417c-af98-d2f0e0a69e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new model RF_tuned2\n",
    "RF_tuned2 = RandomForestRegressor(**RF_tuned.get_params())  # Copy the previous hyperparameters\n",
    "\n",
    "mse_list_rf_fs = []\n",
    "mae_list_rf_fs = []\n",
    "r2_list_rf_fs = []\n",
    "\n",
    "for train_index, val_index in kf.split(x_2):  \n",
    "    x_train_fs, x_val_fs = x_2.iloc[train_index], x_2.iloc[val_index]\n",
    "    y_train_fs, y_val_fs = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Train the new RF_tuned2 model on the reduced feature set\n",
    "    RF_tuned2.fit(x_train_fs, y_train_fs)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred_fs = RF_tuned2.predict(x_val_fs)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_val_fs, y_pred_fs)\n",
    "    mae = mean_absolute_error(y_val_fs, y_pred_fs)\n",
    "    r2 = r2_score(y_val_fs, y_pred_fs)\n",
    "\n",
    "    # Store the results\n",
    "    mse_list_rf_fs.append(mse)\n",
    "    mae_list_rf_fs.append(mae)\n",
    "    r2_list_rf_fs.append(r2)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nRF Model after Feature Selection (RF_tuned2):\")\n",
    "print(\"Average MSE:\", np.mean(mse_list_rf_fs), \"Standard Deviation MSE:\", np.std(mse_list_rf_fs))\n",
    "print(\"Average MAE:\", np.mean(mae_list_rf_fs), \"Standard Deviation MAE:\", np.std(mae_list_rf_fs))\n",
    "print(\"Average R²:\", np.mean(r2_list_rf_fs), \"Standard Deviation R²:\", np.std(r2_list_rf_fs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a175d5b-767c-4243-8865-a9b531f5776a",
   "metadata": {},
   "source": [
    "### GBM Performance after Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde50eb9-2be0-471a-b660-9257da878691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new GBM model: GBM_tuned2\n",
    "GBM_tuned2 = GradientBoostingRegressor(**GBM_tuned.get_params())  # Copy the previous hyperparameters\n",
    "\n",
    "# Initialize lists for storing results\n",
    "mse_list_gbm_fs, mae_list_gbm_fs, r2_list_gbm_fs = [], [], []\n",
    "\n",
    "for train_index, val_index in kf.split(x_2):  \n",
    "    x_train_fs, x_val_fs = x_2.iloc[train_index], x_2.iloc[val_index]\n",
    "    y_train_fs, y_val_fs = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Train the new GBM model on the reduced feature set\n",
    "    GBM_tuned2.fit(x_train_fs, y_train_fs)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred_fs = GBM_tuned2.predict(x_val_fs)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse_list_gbm_fs.append(mean_squared_error(y_val_fs, y_pred_fs))\n",
    "    mae_list_gbm_fs.append(mean_absolute_error(y_val_fs, y_pred_fs))\n",
    "    r2_list_gbm_fs.append(r2_score(y_val_fs, y_pred_fs))\n",
    "\n",
    "# Print results after feature selection\n",
    "print(\"\\nGBM Model after Feature Selection (GBM_tuned2):\")\n",
    "print(\"Average MSE:\", np.mean(mse_list_gbm_fs), \"Standard Deviation MSE:\", np.std(mse_list_gbm_fs))\n",
    "print(\"Average MAE:\", np.mean(mae_list_gbm_fs), \"Standard Deviation MAE:\", np.std(mae_list_gbm_fs))\n",
    "print(\"Average R²:\", np.mean(r2_list_gbm_fs), \"Standard Deviation R²:\", np.std(r2_list_gbm_fs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7895f77-4122-4f00-90fd-9cb710d8c7e2",
   "metadata": {},
   "source": [
    "### FNN Performance after Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe1719-29d4-42e7-9a61-52a1e56da1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy the FNN model\n",
    "FNN_tuned2 = MLPRegressor(**FNN_tuned.get_params()) # Copy the previous parameters\n",
    "\n",
    "# Cross-validation loop for the copied model\n",
    "mse_list_fnn2, mae_list_fnn2, r2_list_fnn2 = [], [], []\n",
    "\n",
    "for train_index, val_index in kf.split(x_2):  \n",
    "    x_train, x_val = x_2.iloc[train_index], x_2.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Train the copied FNN model\n",
    "    FNN_tuned2.fit(x_train, y_train)\n",
    "    y_pred = FNN_tuned2.predict(x_val)\n",
    "    \n",
    "    # Evaluate the copied model\n",
    "    mse_list_fnn2.append(mean_squared_error(y_val, y_pred))\n",
    "    mae_list_fnn2.append(mean_absolute_error(y_val, y_pred))\n",
    "    r2_list_fnn2.append(r2_score(y_val, y_pred))\n",
    "\n",
    "# Print evaluation results for FNN_tuned2\n",
    "print(\"\\nFNN_tuned2 Evaluation:\")\n",
    "print(\"Mean MSE:\", np.mean(mse_list_fnn2), \"Standard Deviation MSE:\", np.std(mse_list_fnn2))\n",
    "print(\"Mean MAE:\", np.mean(mae_list_fnn2), \"Standard Deviation MAE:\", np.std(mae_list_fnn2))\n",
    "print(\"Mean R²:\", np.mean(r2_list_fnn2), \"Standard Deviation R²:\", np.std(r2_list_fnn2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b3f95-ac16-45a9-9b1a-e1b516b778bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Final Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d8892-2c5c-454a-a0ce-b01cd1ce68bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the reduced feature set x_2 into training and test sets\n",
    "x_train_fs, x_test_fs, y_train_fs, y_test_fs = train_test_split(x_2, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# List of tuned models\n",
    "models = {\n",
    "    \"Linear Regression\": LR_model_tuned2,\n",
    "    \"Random Forest\": RF_tuned2,\n",
    "    \"Feedforward Neural Network\": FNN_tuned2,\n",
    "    \"Gradient Boosting Machine\": GBM_tuned2\n",
    "}\n",
    "\n",
    "# Dictionary to store evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Final evaluation on the test set\n",
    "for model_name, model in models.items():\n",
    "    # Train the model on the reduced feature set\n",
    "    model.fit(x_train_fs, y_train_fs)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_fs = model.predict(x_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test_fs, y_pred_fs)\n",
    "    mae = mean_absolute_error(y_test_fs, y_pred_fs)\n",
    "    r2 = r2_score(y_test_fs, y_pred_fs)\n",
    "    \n",
    "    # Store results in a dictionary\n",
    "    evaluation_results[model_name] = {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R²\": r2\n",
    "    }\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"\\nFinal Evaluation Results on Test Set with Reduced Features & Hyperparameters:\")\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  MSE: {results['MSE']:.4f}\")\n",
    "    print(f\"  MAE: {results['MAE']:.4f}\")\n",
    "    print(f\"  R²: {results['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fabb4-73f8-466a-8a07-8c95bb15beb8",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987d4f9-d2b7-46ad-bba0-465282e2e17f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Error analysis of observed and predicted CO2 emission for training (after hyperparameter tuning & feature selection) and test data \n",
    "def plot_scatter_observed_vs_predicted(model_name, y_actual, y_pred, set_type):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_actual, y_pred, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], color='red', linestyle='--', label=\"Ideal Prediction\")\n",
    "    plt.xlabel(\"Observed CO2 Emissions\")\n",
    "    plt.ylabel(\"Predicted CO2 Emissions\")\n",
    "    plt.title(f\"{model_name} - Observed vs Predicted ({set_type} Set)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_Observed_vs_Predicted_{set_type}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Generate and save scatterplots for all models\n",
    "for model_name, model in models.items():\n",
    "    # Train set\n",
    "    y_pred_train = model.predict(x_train_fs)\n",
    "    plot_scatter_observed_vs_predicted(model_name, y_train_fs, y_pred_train, \"Train\")\n",
    "    \n",
    "    # Test set\n",
    "    y_pred_test = model.predict(x_test_fs)\n",
    "    plot_scatter_observed_vs_predicted(model_name, y_test_fs, y_pred_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818431c3-6e2a-4abc-8682-4279abd4bfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform error analysis for each model\n",
    "error_analysis = {}\n",
    "for model_name, model in models.items():\n",
    "    # Predictions on the train set\n",
    "    y_pred_train = model.predict(x_train_fs)\n",
    "    residuals_train = y_train_fs - y_pred_train  # Residuals for training set\n",
    "    \n",
    "    # Predictions on the test set\n",
    "    y_pred_test = model.predict(x_test_fs)\n",
    "    residuals_test = y_test_fs - y_pred_test  # Residuals for test set\n",
    "\n",
    "    # Compute error metrics\n",
    "    mse_train = mean_squared_error(y_train_fs, y_pred_train)\n",
    "    mae_train = mean_absolute_error(y_train_fs, y_pred_train)\n",
    "    r2_train = r2_score(y_train_fs, y_pred_train)\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test_fs, y_pred_test)\n",
    "    mae_test = mean_absolute_error(y_test_fs, y_pred_test)\n",
    "    r2_test = r2_score(y_test_fs, y_pred_test)\n",
    "    \n",
    "    # Store error metrics in dictionary\n",
    "    error_analysis[model_name] = {\n",
    "        \"Train MSE\": mse_train, \"Test MSE\": mse_test,\n",
    "        \"Train MAE\": mae_train, \"Test MAE\": mae_test,\n",
    "        \"Train R²\": r2_train, \"Test R²\": r2_test\n",
    "    }\n",
    "\n",
    "    # Plot observed vs. predicted for train and test sets\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_train_fs, y_pred_train, color=\"skyblue\", alpha=0.7, label=\"Train\")\n",
    "    plt.scatter(y_test_fs, y_pred_test, color=\"coral\", alpha=0.7, label=\"Test\")\n",
    "    plt.plot([y_train_fs.min(), y_train_fs.max()], [y_train_fs.min(), y_train_fs.max()], color='black', linestyle='--', label=\"Ideal\")\n",
    "    plt.xlabel(\"Observed CO2 Emissions\", fontsize=14)\n",
    "    plt.ylabel(\"Predicted CO2 Emissions\", fontsize=14)\n",
    "    plt.title(f\"{model_name} - Observed vs Predicted\", fontsize=14.5)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_Observed_vs_Predicted.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot residuals for training and test sets\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_train_fs, residuals_train, alpha=0.7, color=\"skyblue\", label=\"Train\")\n",
    "    plt.scatter(y_test_fs, residuals_test, alpha=0.7, color=\"coral\", label=\"Test\")\n",
    "    plt.axhline(0, color='black', linestyle='--', label=\"Zero Residual\")\n",
    "    plt.xlabel(\"Observed CO2 Emissions\", fontsize=14)\n",
    "    plt.ylabel(\"Residuals (Observed - Predicted)\", fontsize=14)\n",
    "    plt.title(f\"{model_name} - Residuals Scatterplot\", fontsize=14.5)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_Residuals_Scatterplot.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Print the summary of error metrics\n",
    "print(\"\\nError Analysis Summary:\")\n",
    "for model_name, metrics in error_analysis.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
